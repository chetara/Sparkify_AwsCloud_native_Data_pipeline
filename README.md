# ğŸ§ Sparkify Cloud-Native Data Pipeline

This project builds a scalable, cloud-native data pipeline for a fictional music streaming company, **Sparkify**, using AWS and Apache Spark. It extracts raw JSON data from S3, transforms it with PySpark, and loads it into a structured **data lake** in Parquet format.

---

## ğŸ“Œ Project Goals

- Build a modern data lake architecture
- Process semi-structured data with Apache Spark
- Store transformed data in AWS S3 as Parquet
- Lay foundation for orchestration (Airflow), querying (Athena/Redshift), and containerization (Docker)

---

## âœ… Phase 1: Plan the Architecture

### ğŸ¯ What & Why

Cloud-native, serverless, and cost-efficient:
- **S3** for raw & transformed data (acts as the data lake)
- **Spark (on local or EMR)** for distributed processing
- **Parquet** for optimized columnar storage
- Optional: Redshift, Athena, or Quicksight for analytics

### ğŸ—‚ï¸ Folder Structure
```bash
Sparkify_Cloud_Native-pipeline/
â”‚
â”œâ”€â”€ raw_data/                      # Raw input data
â”‚   â”œâ”€â”€ song_data/                 # Raw song data (JSON format)
â”‚   â””â”€â”€ log_data/                  # Raw log data (JSON format)
â”‚
â”œâ”€â”€ spark_jobs/                    # PySpark ETL job files
â”‚   â”œâ”€â”€ spark_etl.py               # The PySpark ETL job that loads, transforms, and writes data
â”‚   â””â”€â”€ other_etl_jobs.py          # (Optional) Other Spark jobs (e.g., batch processing, transformations)
â”‚
â”œâ”€â”€ output/                        # (Optional) Local output storage for Parquet files (for testing)
â”‚   â”œâ”€â”€ users/                     # Parquet files for the users table
â”‚   â”œâ”€â”€ songs/                     # Parquet files for the songs table
â”‚   â”œâ”€â”€ artists/                   # Parquet files for the artists table
â”‚   â””â”€â”€ songplays/                 # Parquet files for the songplays table
â”‚
â”œâ”€â”€ s3/                            # Folder to store S3 configurations (if necessary)
â”‚   â”œâ”€â”€ s3_config.json             # Configuration for S3 bucket setup
â”‚   â””â”€â”€ dwh.cfg                    # AWS configuration file (with credentials and S3 bucket info)
â”‚
â”‚
â”œâ”€â”€ Docker/          
â”‚
â”œâ”€â”€ scripts/                        # Utility or helper scripts for running the project
â”‚   â”œâ”€â”€ run_etl.sh                 # Shell script to execute the PySpark ETL job (locally or remotely)
â”‚   â””â”€â”€ deploy.sh                  # (Optional) Script to deploy the project on AWS or other platforms
â”‚
â”œâ”€â”€ README.md                      # Project documentation
â””â”€â”€ requirements.txt               # Python dependencies (e.g., pyspark, boto3, pandas)
```


---

## âœ… Phase 2: Initialize the Project

### ğŸ”§ Setup

1. âœ… Create and connect GitHub repo  
2. âœ… Configure AWS CLI  
3. âœ… Set up `dwh.cfg` with:
   - AWS credentials
   - S3 bucket paths
   - Region

### ğŸª£ S3 Buckets Used

| Bucket Name                 | Purpose                |
|-----------------------------|------------------------|
| `sparkify-datalake-aws`     | Raw and processed data |
| `sparkify-datalake-athena-results` | Athena output (optional) |

### ğŸ“ S3 Structure


s3://sparkify-datalake-aws/
``` bash 
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ song_data/                 # Raw song data (JSON format)
â”‚   â””â”€â”€ log_data/                  # Raw log data (JSON format)
â”‚
â”œâ”€â”€ curated/                        # Processed/Transformed Data
â”‚   â”œâ”€â”€ songs/                     # Processed song data (Parquet format)
â”‚   â”œâ”€â”€ users/                     # Processed user data (Parquet format)
â”‚   â”œâ”€â”€ artists/                   # Processed artist data (Parquet format)
â”‚   â””â”€â”€ songplays/                 # Processed songplay data (Parquet format)
â”‚

```


## âœ… Phase 3: Build & Run Spark ETL

### ğŸ”¥ ETL Overview

- Load raw `song_data` and `log_data` from S3
- Transform into dimensional model (`users`, `songs`, `artists`, `time`, `songplays`)
- Write back as Parquet into curated S3 paths

### âš™ï¸ Technologies Used

- [Apache Spark](https://spark.apache.org/) (PySpark)
- AWS S3 (`s3a://` access via `hadoop-aws`)
- Pandas (for local verification)
- JSON, Parquet

### ğŸš€ Run Spark Locally

```bash
spark-submit \
  --packages org.apache.hadoop:hadoop-aws:3.2.2 \
  spark_jobs/spark_etl.py

```

## ğŸ‘¨â€ğŸ’» Author
Chetara AbdelOuahab
Cloud Data Engineer in progress â˜ï¸
Project repo: github.com/chetara/Sparkify_Cloud_Native-pipeline