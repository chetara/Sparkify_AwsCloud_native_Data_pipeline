# ğŸ§ Sparkify Cloud-Native Data Pipeline

This project builds a scalable, cloud-native data pipeline for a fictional music streaming company, **Sparkify**, using AWS and Apache Spark. It extracts raw JSON data from S3, transforms it with PySpark, and loads it into a structured **data lake** in Parquet format.

---

## ğŸ“Œ Project Goals

- Build a modern data lake architecture
- Process semi-structured data with Apache Spark
- Store transformed data in AWS S3 as Parquet
- Lay foundation for orchestration (Airflow), querying (Athena/Redshift), and containerization (Docker)

---

## âœ… Phase 1: Plan the Architecture

### ğŸ¯ What & Why

Cloud-native, serverless, and cost-efficient:
- **S3** for raw & transformed data (acts as the data lake)
- **Spark (on local or EMR)** for distributed processing
- **Parquet** for optimized columnar storage
- Optional: Redshift, Athena, or Quicksight for analytics

### ğŸ—‚ï¸ Folder Structure
```bash
Sparkify_Cloud_Native-pipeline/
â”‚
â”œâ”€â”€ raw_data/                      # Raw input data
â”‚   â”œâ”€â”€ song_data/                 # Raw song data (JSON format)
â”‚   â””â”€â”€ log_data/                  # Raw log data (JSON format)
â”‚
â”œâ”€â”€ spark_jobs/                    # PySpark ETL job files
â”‚   â”œâ”€â”€ spark_etl.py               # The PySpark ETL job that loads, transforms, and writes data
â”‚   â””â”€â”€ other_etl_jobs.py          # (Optional) Other Spark jobs (e.g., batch processing, transformations)
â”‚
â”œâ”€â”€ output/                        # (Optional) Local output storage for Parquet files (for testing)
â”‚   â”œâ”€â”€ users/                     # Parquet files for the users table
â”‚   â”œâ”€â”€ songs/                     # Parquet files for the songs table
â”‚   â”œâ”€â”€ artists/                   # Parquet files for the artists table
â”‚   â””â”€â”€ songplays/                 # Parquet files for the songplays table
â”‚
â”œâ”€â”€ s3/                            # Folder to store S3 configurations (if necessary)
â”‚   â”œâ”€â”€ s3_config.json             # Configuration for S3 bucket setup
â”‚   â””â”€â”€ dwh.cfg                    # AWS configuration file (with credentials and S3 bucket info)
â”‚
â”‚
â”œâ”€â”€ Docker/          
â”‚
â”œâ”€â”€ scripts/                        # Utility or helper scripts for running the project
â”‚   â”œâ”€â”€ run_etl.sh                 # Shell script to execute the PySpark ETL job (locally or remotely)
â”‚   â””â”€â”€ deploy.sh                  # (Optional) Script to deploy the project on AWS or other platforms
â”‚
â”œâ”€â”€ README.md                      # Project documentation
â””â”€â”€ requirements.txt               # Python dependencies (e.g., pyspark, boto3, pandas)
```


---

## âœ… Phase 2: Initialize the Project

### ğŸ”§ Setup

1. âœ… Create and connect GitHub repo  
2. âœ… Configure AWS CLI  
3. âœ… Set up `dwh.cfg` with:
   - AWS credentials
   - S3 bucket paths
   - Region

### ğŸª£ S3 Buckets Used

| Bucket Name                 | Purpose                |
|-----------------------------|------------------------|
| `sparkify-datalake-aws`     | Raw and processed data |
| `sparkify-datalake-athena-results` | Athena output (optional) |

### ğŸ“ S3 Structure


s3://sparkify-datalake-aws/
``` bash 
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ song_data/                 # Raw song data (JSON format)
â”‚   â””â”€â”€ log_data/                  # Raw log data (JSON format)
â”‚
â”œâ”€â”€ curated/                        # Processed/Transformed Data
â”‚   â”œâ”€â”€ songs/                     # Processed song data (Parquet format)
â”‚   â”œâ”€â”€ users/                     # Processed user data (Parquet format)
â”‚   â”œâ”€â”€ artists/                   # Processed artist data (Parquet format)
â”‚   â””â”€â”€ songplays/                 # Processed songplay data (Parquet format)
â”‚

```


## âœ… Phase 3: Build & Run Spark ETL

### ğŸ”¥ ETL Overview

- Load raw `song_data` and `log_data` from S3
- Transform into dimensional model (`users`, `songs`, `artists`, `time`, `songplays`)
- Write back as Parquet into curated S3 paths

### âš™ï¸ Technologies Used

- [Apache Spark](https://spark.apache.org/) (PySpark)
- AWS S3 (`s3a://` access via `hadoop-aws`)
- Pandas (for local verification)
- JSON, Parquet

### ğŸš€ Run Spark Locally

```bash
spark-submit \
  --packages org.apache.hadoop:hadoop-aws:3.2.2 \
  spark_jobs/spark_etl.py
  ```

##  Phase 4: Containerize with Docker
ğŸ³ Why Docker?

    Ensure consistency across environments

    Simplify dependency management

    Enable scalable orchestration (Airflow, Spark) with isolated services

âš™ï¸ Setup

Your project includes a docker-compose.yml file that defines:
Service	Purpose
etl	Runs the Spark ETL job
spark-master	Spark master node (local cluster)
spark-worker	Spark worker node
postgres	Metadata database for Airflow
airflow-webserver	Airflow UI (on port 8080)
airflow-scheduler	DAG scheduler
ğŸ—‚ Docker Folder Structure

Docker/
â”œâ”€â”€ Dockerfile                # Base image for Spark & PySpark ETL
â””â”€â”€ .env                      # Secrets (AWS keys, Fernet key)

ğŸš€ Run with Docker

# Step 1: Initialize Airflow DB (first-time only)
``` bash 
docker-compose run --rm airflow-webserver airflow db init
```
# Step 2: Launch all services
``` bash 
docker-compose up --build
```

Go to http://localhost:8080 to access Airflow.
âœ… Phase 5: Orchestrate with Airflow
ğŸ›ï¸ What is Airflow?

Apache Airflow is used to schedule and monitor workflows. In this project, it triggers the PySpark ETL job daily.
# Step 2: Launch all services
âš™ï¸ DAG Configuration

DAG file: dags/sparkify_etl_dag.py

    Uses BashOperator to call the Spark job:
``` python
bash_command='python /app/spark_etl.py'
```
    Passes AWS credentials from Airflow variables:
``` python
env={
  'AWS_ACCESS_KEY_ID': '{{ var.value.AWS_ACCESS_KEY_ID }}',
  'AWS_SECRET_ACCESS_KEY': '{{ var.value.AWS_SECRET_ACCESS_KEY }}',
  'AWS_REGION': '{{ var.value.AWS_REGION }}'
}
```
âœ… Airflow Steps

    Add secrets (in UI):

        AWS_ACCESS_KEY_ID

        AWS_SECRET_ACCESS_KEY

        AWS_REGION

    Ensure Fernet key is set in .env and used in docker-compose.yml:
``` env
FERNET_KEY=generated_fernet_key_here
```
    Trigger DAG manually or wait for schedule.



## ğŸ‘¨â€ğŸ’» Author
Chetara AbdelOuahab
Cloud Data Engineer in progress â˜ï¸
Project repo: github.com/chetara/Sparkify_Cloud_Native-pipeline